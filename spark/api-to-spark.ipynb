{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "import requests\n",
    "\n",
    "import pyspark\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import from_json, explode, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "\n",
    "import pytz\n",
    "from astral import LocationInfo\n",
    "from astral.sun import sun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lulu/spark/spark-3.5.0-bin-hadoop3\n",
      "/home/lulu/spark/spark-3.5.0-bin-hadoop3/lib/gcs-connector-hadoop3-latest.jar\n"
     ]
    }
   ],
   "source": [
    "credentials_location = '/home/lulu/projects/data_talks/mage-zoomcamp/google_cloud_key.json'\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME')\n",
    "jar_file = os.path.join(spark_home, 'lib', 'gcs-connector-hadoop3-latest.jar')\n",
    "print(spark_home)\n",
    "print(jar_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", jar_file) \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", credentials_location)\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "hadoop_conf = sc._jsc.hadoopConfiguration()\n",
    "\n",
    "hadoop_conf.set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "hadoop_conf.set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.json.keyfile\", credentials_location)\n",
    "hadoop_conf.set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must leave everything as string and cast as int or double later otherwise interpreted as null\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField('collision_id', StringType(), True),\n",
    "    StructField('crash_date', TimestampType(), True),\n",
    "    StructField('crash_time', TimestampType(), True),\n",
    "    StructField('zip_code', StringType(), True),\n",
    "    StructField('borough', StringType(), True),\n",
    "    StructField('latitude', StringType(), True), \n",
    "    StructField('longitude', StringType(), True),\n",
    "    #StructField('location_latitude', DoubleType(), True),  # Expanded latitude field\n",
    "    #StructField('location_longitude', DoubleType(), True),  # Expanded longitude field\n",
    "    #StructField('location_human_address', StringType(), True),  # Expanded human_address field\n",
    "    StructField('on_street_name', StringType(), True),\n",
    "    StructField('off_street_name', StringType(), True),\n",
    "    StructField('cross_street_name', StringType(), True),\n",
    "    StructField('number_of_persons_injured', StringType(), True),\n",
    "    StructField('number_of_pedestrians_injured', StringType(), True),\n",
    "    StructField('number_of_cyclist_injured', StringType(), True),\n",
    "    StructField('number_of_motorist_injured', StringType(), True),\n",
    "    StructField('number_of_persons_killed', StringType(), True),\n",
    "    StructField('number_of_pedestrians_killed', StringType(), True),\n",
    "    StructField('number_of_cyclist_killed', StringType(), True),\n",
    "    StructField('number_of_motorist_killed', StringType(), True),\n",
    "    StructField('contributing_factor_vehicle_1', StringType(), True),\n",
    "    StructField('contributing_factor_vehicle_2', StringType(), True),\n",
    "    StructField('contributing_factor_vehicle_3', StringType(), True),\n",
    "    StructField('contributing_factor_vehicle_4', StringType(), True),\n",
    "    StructField('contributing_factor_vehicle_5', StringType(), True),\n",
    "    StructField('vehicle_type_code1', StringType(), True),\n",
    "    StructField('vehicle_type_code2', StringType(), True),\n",
    "    StructField('vehicle_type_code_3', StringType(), True),\n",
    "    StructField('vehicle_type_code_4', StringType(), True),\n",
    "    StructField('vehicle_type_code_5', StringType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit=200000&$offset=0\n",
      "https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit=200000&$offset=200000\n",
      "https://data.cityofnewyork.us/resource/h9gi-nx95.json?$limit=200000&$offset=400000\n"
     ]
    }
   ],
   "source": [
    "offset = 0\n",
    "batch_num = 0\n",
    "batch_size = 200000\n",
    "batch_data = []\n",
    "\n",
    "\n",
    "api_endpoint = 'https://data.cityofnewyork.us/resource/h9gi-nx95.json'\n",
    "\n",
    "# Fetch JSON data from the API in batches\n",
    "while True:\n",
    "    \n",
    "    # RETRIEVE DATA FROM API ENDPOINT \n",
    "    url = f'{api_endpoint}?$limit={batch_size}&$offset={offset}'\n",
    "    print(url)\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if not response.ok:\n",
    "        raise Exception(f\"Failed to fetch data from API: {response.status_code}\")\n",
    "\n",
    "    # Extract JSON content\n",
    "    json_content = response.json()\n",
    "    \n",
    "    # Convert JSON content to RDD\n",
    "    rdd = spark.sparkContext.parallelize(json_content)\n",
    "    \n",
    "    # Read JSON data into DataFrame\n",
    "    df = spark.read.json(rdd, schema=custom_schema)\n",
    "    \n",
    "    # Append batch DataFrame to the list\n",
    "    batch_data.append(df)\n",
    "    \n",
    "    offset += batch_size\n",
    "    batch_num += 1\n",
    "    \n",
    "    if batch_num == 3:\n",
    "    #if offset > 200000:\n",
    "        break\n",
    "    \n",
    "# Union all batch DataFrames\n",
    "final_df = batch_data[0]\n",
    "for batch_df in batch_data[1:]:\n",
    "    final_df = final_df.union(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/16 15:31:48 WARN TaskSetManager: Stage 5 contains a task of very large size (3834 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 5:=======================================>                (34 + 14) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 600000\n",
      "Number of columns: 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = final_df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(final_df.columns)\n",
    "\n",
    "print(\"Number of rows:\", num_rows)\n",
    "print(\"Number of columns:\", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[UNSUPPORTED_DATATYPE] Unsupported data type \"TIME\".(line 1, pos 0)\n\n== SQL ==\ntime\n^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 21\u001b[0m\n\u001b[1;32m      3\u001b[0m columns_to_cast \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrash_date\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcrash_time\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip_code\u001b[39m\u001b[38;5;124m\"\u001b[39m: IntegerType(),\n\u001b[1;32m     18\u001b[0m }\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col_name, col_type \u001b[38;5;129;01min\u001b[39;00m columns_to_cast\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 21\u001b[0m     final_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mwithColumn(col_name, \u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_name\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_type\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/spark/spark-3.5.0-bin-hadoop3/python/pyspark/sql/column.py:1204\u001b[0m, in \u001b[0;36mColumn.cast\u001b[0;34m(self, dataType)\u001b[0m\n\u001b[1;32m   1174\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1175\u001b[0m \u001b[38;5;124;03mCasts the column into type ``dataType``.\u001b[39;00m\n\u001b[1;32m   1176\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03m[Row(ages='2'), Row(ages='5')]\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataType, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 1204\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataType\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dataType, DataType):\n\u001b[1;32m   1206\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n",
      "File \u001b[0;32m~/spark/spark-3.5.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/spark/spark-3.5.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[UNSUPPORTED_DATATYPE] Unsupported data type \"TIME\".(line 1, pos 0)\n\n== SQL ==\ntime\n^^^\n"
     ]
    }
   ],
   "source": [
    "# CAST THE DATA TYPES\n",
    "\n",
    "columns_to_cast = {\n",
    "    \"crash_date\": \"date\",\n",
    "    \"crash_time\": \"time\",\n",
    "    \"latitude\": DoubleType(),\n",
    "    \"longitude\": DoubleType(),\n",
    "    \"number_of_persons_injured\": IntegerType(),\n",
    "    \"number_of_pedestrians_injured\": IntegerType(),\n",
    "    \"number_of_cyclist_injured\": IntegerType(),\n",
    "    \"number_of_motorist_injured\": IntegerType(),\n",
    "    \"number_of_persons_killed\": IntegerType(),\n",
    "    \"number_of_pedestrians_killed\": IntegerType(),\n",
    "    \"number_of_cyclist_killed\": IntegerType(),\n",
    "    \"number_of_motorist_killed\": IntegerType(),\n",
    "    \"collision_id\": IntegerType(),\n",
    "    \"zip_code\": IntegerType(),\n",
    "}\n",
    "    \n",
    "for col_name, col_type in columns_to_cast.items():\n",
    "    final_df = final_df.withColumn(col_name, col(col_name).cast(col_type))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType([StructField('collision_id', IntegerType(), True), StructField('crash_date', TimestampType(), True), StructField('crash_time', TimestampType(), True), StructField('zip_code', IntegerType(), True), StructField('borough', StringType(), True), StructField('latitude', DoubleType(), True), StructField('longitude', DoubleType(), True), StructField('on_street_name', StringType(), True), StructField('off_street_name', StringType(), True), StructField('cross_street_name', StringType(), True), StructField('number_of_persons_injured', IntegerType(), True), StructField('number_of_pedestrians_injured', IntegerType(), True), StructField('number_of_cyclist_injured', IntegerType(), True), StructField('number_of_motorist_injured', IntegerType(), True), StructField('number_of_persons_killed', IntegerType(), True), StructField('number_of_pedestrians_killed', IntegerType(), True), StructField('number_of_cyclist_killed', IntegerType(), True), StructField('number_of_motorist_killed', IntegerType(), True), StructField('contributing_factor_vehicle_1', StringType(), True), StructField('contributing_factor_vehicle_2', StringType(), True), StructField('contributing_factor_vehicle_3', StringType(), True), StructField('contributing_factor_vehicle_4', StringType(), True), StructField('contributing_factor_vehicle_5', StringType(), True), StructField('vehicle_type_code1', StringType(), True), StructField('vehicle_type_code2', StringType(), True), StructField('vehicle_type_code_3', StringType(), True), StructField('vehicle_type_code_4', StringType(), True), StructField('vehicle_type_code_5', StringType(), True)])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------------+-------------------+--------+--------+---------+----------+--------------------+---------------+--------------------+-------------------------+-----------------------------+-------------------------+--------------------------+------------------------+----------------------------+------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|collision_id|         crash_date|         crash_time|zip_code| borough| latitude| longitude|      on_street_name|off_street_name|   cross_street_name|number_of_persons_injured|number_of_pedestrians_injured|number_of_cyclist_injured|number_of_motorist_injured|number_of_persons_killed|number_of_pedestrians_killed|number_of_cyclist_killed|number_of_motorist_killed|contributing_factor_vehicle_1|contributing_factor_vehicle_2|contributing_factor_vehicle_3|contributing_factor_vehicle_4|contributing_factor_vehicle_5|vehicle_type_code1|vehicle_type_code2|vehicle_type_code_3|vehicle_type_code_4|vehicle_type_code_5|\n",
      "+------------+-------------------+-------------------+--------+--------+---------+----------+--------------------+---------------+--------------------+-------------------------+-----------------------------+-------------------------+--------------------------+------------------------+----------------------------+------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "|        NULL|2021-09-11 00:00:00|2024-04-16 02:39:00|    NULL|    NULL|     NULL|      NULL|WHITESTONE EXPRES...|      20 AVENUE|                NULL|                        2|                            0|                        0|                         2|                       0|                           0|                       0|                        0|         Aggressive Drivin...|                  Unspecified|                         NULL|                         NULL|                         NULL|             Sedan|             Sedan|               NULL|               NULL|               NULL|\n",
      "|        NULL|2022-03-26 00:00:00|2024-04-16 11:45:00|    NULL|    NULL|     NULL|      NULL|QUEENSBORO BRIDGE...|           NULL|                NULL|                        1|                            0|                        0|                         1|                       0|                           0|                       0|                        0|            Pavement Slippery|                         NULL|                         NULL|                         NULL|                         NULL|             Sedan|              NULL|               NULL|               NULL|               NULL|\n",
      "|        NULL|2022-06-29 00:00:00|2024-04-16 06:55:00|    NULL|    NULL|     NULL|      NULL|  THROGS NECK BRIDGE|           NULL|                NULL|                        0|                            0|                        0|                         0|                       0|                           0|                       0|                        0|         Following Too Clo...|                  Unspecified|                         NULL|                         NULL|                         NULL|             Sedan|     Pick-up Truck|               NULL|               NULL|               NULL|\n",
      "|        NULL|2021-09-11 00:00:00|2024-04-16 09:35:00|   11208|BROOKLYN|40.667202|  -73.8665|                NULL|           NULL|1211      LORING ...|                        0|                            0|                        0|                         0|                       0|                           0|                       0|                        0|                  Unspecified|                         NULL|                         NULL|                         NULL|                         NULL|             Sedan|              NULL|               NULL|               NULL|               NULL|\n",
      "|        NULL|2021-12-14 00:00:00|2024-04-16 08:13:00|   11233|BROOKLYN|40.683304|-73.917274|     SARATOGA AVENUE| DECATUR STREET|                NULL|                        0|                            0|                        0|                         0|                       0|                           0|                       0|                        0|                         NULL|                         NULL|                         NULL|                         NULL|                         NULL|              NULL|              NULL|               NULL|               NULL|               NULL|\n",
      "+------------+-------------------+-------------------+--------+--------+---------+----------+--------------------+---------------+--------------------+-------------------------+-----------------------------+-------------------------+--------------------------+------------------------+----------------------------+------------------------+-------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+-----------------------------+------------------+------------------+-------------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/16 15:34:28 WARN TaskSetManager: Stage 8 contains a task of very large size (3834 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    }
   ],
   "source": [
    "final_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "de_zoom",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
